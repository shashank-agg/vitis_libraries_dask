{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress, get_worker\n",
    "import os\n",
    "client = Client(\"tcp://131.180.106.138:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_TO_BE_COMPRESSED = \"sample.txt\"\n",
    "TEMP_FILE_ON_WORKER = \"_worker/temp.txt\"\n",
    "TEMP_COMPRESSED_FILE_ON_WORKER = \"_worker/temp.gz\"\n",
    "FINAL_COMPRESSED_FILE = \"compressed.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_on_worker(file_bytes, index):\n",
    "    import requests\n",
    "    from xfZlibWrapper import xfZlibWrapper\n",
    "    \n",
    "    #Avoid reinstalling the xclbin\n",
    "    if not hasattr(get_worker(), '_xclbin'): \n",
    "        xfZlib = xfZlibWrapper(b\"/shares/bulk/shashankaggarw/compress_decompress.xclbin\")\n",
    "        get_worker()._xclbin = xfZlib\n",
    "    else:\n",
    "        xfZlib = get_worker()._xclbin\n",
    "    \n",
    "    os.makedirs(os.path.dirname(\"_worker/\"), exist_ok=True)    \n",
    "    open(TEMP_FILE_ON_WORKER, 'wb').write(file_bytes)\n",
    "    \n",
    "    # Call FPGA driver\n",
    "    print(\"calling constructor\")\n",
    "    xfZlib = xfZlibWrapper(b\"/shares/bulk/shashankaggarw/compress_decompress.xclbin\")\n",
    "\n",
    "    size = xfZlib.compress_file(TEMP_FILE_ON_WORKER, TEMP_COMPRESSED_FILE_ON_WORKER)\n",
    "    print('Compressed from ', os.path.getsize(TEMP_FILE_ON_WORKER),' to ', size, ' bytes')\n",
    "    f = open(TEMP_COMPRESSED_FILE_ON_WORKER, \"rb\")\n",
    "    return {\n",
    "        'index': index,\n",
    "        'data': f.read()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting input file into 1 chunk(s)\n",
      "Received data from workers\n",
      "Merging the compressed files received\n",
      "Writing combined (compressed) data to compressed.gz\n",
      "TOTAL EXECUTION TIME:  0.4071033000946045 498\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "\n",
    "num_of_workers = len(client.scheduler_info()[\"workers\"])\n",
    "data_split = []\n",
    "\n",
    "# Split up the file into equal sized chunks based on number of available dask workers\n",
    "\n",
    "print(\"Splitting input file into\", num_of_workers, \"chunk(s)\")\n",
    "with open(FILE_TO_BE_COMPRESSED, \"rb\") as ifile:    \n",
    "    total = ifile.read()\n",
    "    start = 0\n",
    "    chunk_size = int(len(total)/num_of_workers)\n",
    "    for i in range(num_of_workers):\n",
    "        data_split.append(total[start: start+chunk_size])\n",
    "        start += chunk_size\n",
    "    \n",
    "# print(data_split)\n",
    "# Scatter the data to the workers before calling run_on_worker on the workers\n",
    "distributed_data = client.scatter(data_split)\n",
    "futures = client.map(run_on_worker, distributed_data, range(num_of_workers))\n",
    "results = client.gather(futures)\n",
    "print(\"Received data from workers\")\n",
    "\n",
    "results.sort(key = lambda result: result['index'])  # Reorder the response\n",
    "results = [r['data'] for r in results]\n",
    "\n",
    "gzip_header_size = 10 + len(TEMP_FILE_ON_WORKER) + 1 # standard header + file name + \"\\0\"\n",
    "gzip_footer_size = 8 + 5 # Standard footer + 5 bytes padding added by xfZlib\n",
    "\n",
    "print(\"Merging the compressed files received\")\n",
    "concatenated_gzip_bytes = results[0][0:gzip_header_size] # Header from first result\n",
    "for result in results[0:]:\n",
    "    concatenated_gzip_bytes += result[gzip_header_size:-1*gzip_footer_size]\n",
    "concatenated_gzip_bytes += results[0][-1*gzip_header_size:] # Footer from first result\n",
    "\n",
    "print(\"Writing combined (compressed) data to \" + FINAL_COMPRESSED_FILE)\n",
    "with open(FINAL_COMPRESSED_FILE, \"wb\") as f:\n",
    "    f.write(concatenated_gzip_bytes)\n",
    "t1 = time.time()\n",
    "print(\"TOTAL EXECUTION TIME (in s): \", t1 - t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting compressed.gz using command: \n",
      "gzip -dc compressed.gz > sample.txt.copy\n",
      "Comparing sample.txt.copy to sample.txt\n",
      "Validation succeeded !!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FILE_COPY = FILE_TO_BE_COMPRESSED + \".copy\"\n",
    "COMMAND_TO_RUN = \"gzip -dc \" + FINAL_COMPRESSED_FILE + \" > \" + FILE_COPY\n",
    "print(\"Extracting\", FINAL_COMPRESSED_FILE, \"using command: \")\n",
    "print(COMMAND_TO_RUN)\n",
    "os.system(COMMAND_TO_RUN)\n",
    "print(\"Comparing\", FILE_COPY, \"to\", FILE_TO_BE_COMPRESSED)\n",
    "with open(FILE_TO_BE_COMPRESSED, 'rb') as f1:\n",
    "    with open(FILE_COPY, 'rb') as f2:\n",
    "        if f1.read() == f2.read():\n",
    "            print(\"Validation succeeded !!\")\n",
    "        else:\n",
    "            print(\"Validation failed !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
